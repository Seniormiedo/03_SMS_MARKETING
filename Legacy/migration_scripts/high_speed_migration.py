#!/usr/bin/env python3
"""
MIGRACI√ìN DE ALTA VELOCIDAD - SOLUCIONES REALES
Implementa t√©cnicas profesionales para migraci√≥n masiva r√°pida
"""

import sqlite3
import subprocess
import sys
import os
import time
import json
import psutil
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
import csv
import io

# Agregar el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.migration_manager import DataTransformer

class HighSpeedMigration:
    """Migraci√≥n de alta velocidad usando t√©cnicas profesionales"""
    
    def __init__(self):
        self.start_time = None
        self.total_records = 0
        self.processed_records = 0
        self.successful_inserts = 0
        self.failed_inserts = 0
        self.batch_size = 100000  # 100K registros por lote
        
    def log_message(self, message, level="INFO"):
        """Log con timestamp"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def check_system_resources(self):
        """Verificar recursos del sistema"""
        self.log_message("üîç Verificando recursos del sistema...")
        
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        memory_available_gb = memory.available / (1024**3)
        
        disk = psutil.disk_usage('.')
        disk_free_gb = disk.free / (1024**3)
        
        self.log_message(f"üíæ Memoria: {memory_available_gb:.1f}GB disponible de {memory_gb:.1f}GB total")
        self.log_message(f"üíΩ Disco: {disk_free_gb:.1f}GB libres")
        
        return disk_free_gb >= 40
    
    def get_total_records(self):
        """Obtener conteo exacto de registros"""
        self.log_message("üìä Contando registros totales en numeros.db...")
        
        try:
            conn = sqlite3.connect("numeros.db")
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM numeros")
            count = cursor.fetchone()[0]
            conn.close()
            
            self.total_records = count
            total_batches = (count + self.batch_size - 1) // self.batch_size
            
            self.log_message(f"üì± Total de registros: {count:,}")
            self.log_message(f"üì¶ Total de lotes: {total_batches:,} (tama√±o: {self.batch_size:,})")
            
            return count
        except Exception as e:
            self.log_message(f"‚ùå ERROR contando registros: {e}", "ERROR")
            return 0
    
    def optimize_postgresql_aggressive(self):
        """Optimizaci√≥n agresiva de PostgreSQL para m√°xima velocidad"""
        self.log_message("‚ö° Aplicando optimizaci√≥n agresiva PostgreSQL...")
        
        optimization_commands = [
            # Deshabilitar completamente WAL y checkpoints
            "ALTER SYSTEM SET wal_level = minimal;",
            "ALTER SYSTEM SET synchronous_commit = off;",
            "ALTER SYSTEM SET fsync = off;",
            "ALTER SYSTEM SET full_page_writes = off;",
            "ALTER SYSTEM SET checkpoint_segments = 32;",
            "ALTER SYSTEM SET checkpoint_completion_target = 0.9;",
            "ALTER SYSTEM SET wal_buffers = '64MB';",
            "ALTER SYSTEM SET shared_buffers = '1GB';",
            "ALTER SYSTEM SET work_mem = '512MB';",
            "ALTER SYSTEM SET maintenance_work_mem = '2GB';",
            "ALTER SYSTEM SET effective_cache_size = '4GB';",
            "ALTER SYSTEM SET random_page_cost = 1.1;",
            "ALTER SYSTEM SET autovacuum = off;",
            "SELECT pg_reload_conf();"
        ]
        
        for cmd in optimization_commands:
            try:
                subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-c', cmd
                ], capture_output=True, text=True, timeout=10)
            except:
                pass
        
        self.log_message("‚úÖ Optimizaci√≥n agresiva aplicada")
    
    def clear_target_table(self):
        """Limpiar tabla de destino"""
        self.log_message("üßπ Limpiando tabla contacts...")
        
        try:
            result = subprocess.run([
                'docker-compose', 'exec', '-T', 'postgres',
                'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                '-c', 'TRUNCATE TABLE contacts RESTART IDENTITY CASCADE;'
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                self.log_message("‚úÖ Tabla contacts limpiada")
                return True
            else:
                self.log_message(f"‚ùå Error limpiando tabla: {result.stderr}", "ERROR")
                return False
        except Exception as e:
            self.log_message(f"‚ùå Error limpiando tabla: {e}", "ERROR")
            return False
    
    def method_1_psql_copy_stdin(self, batch_number, offset, limit):
        """M√âTODO 1: psql \\copy desde STDIN (m√°s r√°pido que COPY FROM archivo)"""
        batch_start = time.time()
        total_batches = (self.total_records + self.batch_size - 1) // self.batch_size
        self.log_message(f"üöÄ M√âTODO 1 - Lote {batch_number}/{total_batches} (registros {offset+1:,} - {offset+limit:,})")
        
        try:
            # Obtener datos de SQLite
            conn = sqlite3.connect("numeros.db")
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute(f"SELECT * FROM numeros LIMIT {limit} OFFSET {offset}")
            batch_records = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            if not batch_records:
                return 0
            
            # Transformar registros
            transformer = DataTransformer()
            csv_data = io.StringIO()
            csv_writer = csv.writer(csv_data, delimiter='\t', quoting=csv.QUOTE_MINIMAL)
            
            valid_records = 0
            for record in batch_records:
                result = transformer.transform_record(record)
                if result["success"]:
                    data = result["data"]
                    
                    # Escribir fila CSV
                    csv_writer.writerow([
                        data.get("phone_e164", ""),
                        data.get("phone_national", ""),
                        data.get("phone_original", ""),
                        data.get("full_name", ""),
                        data.get("address", ""),
                        data.get("neighborhood", ""),
                        data.get("lada", ""),
                        data.get("state_code", ""),
                        data.get("state_name", ""),
                        data.get("municipality", ""),
                        data.get("city", ""),
                        "t" if data.get("is_mobile") else "f",
                        data.get("operator", ""),
                        data.get("status", "UNKNOWN"),
                        "",  # status_updated_at
                        "",  # status_source
                        0,   # send_count
                        "",  # last_sent_at
                        "",  # opt_out_at
                        "",  # opt_out_method
                        "",  # last_validated_at
                        0,   # validation_attempts
                        data.get("source", "TELCEL2022"),
                        ""   # import_batch_id
                    ])
                    valid_records += 1
            
            if valid_records == 0:
                return 0
            
            # Usar psql \copy desde STDIN
            csv_content = csv_data.getvalue()
            
            copy_command = """\\copy contacts (phone_e164, phone_national, phone_original, full_name, address, neighborhood, lada, state_code, state_name, municipality, city, is_mobile, operator, status, status_updated_at, status_source, send_count, last_sent_at, opt_out_at, opt_out_method, last_validated_at, validation_attempts, source, import_batch_id) FROM STDIN WITH (FORMAT csv, DELIMITER E'\\t', NULL '')"""
            
            process = subprocess.Popen([
                'docker-compose', 'exec', '-T', 'postgres',
                'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                '-c', copy_command
            ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            
            stdout, stderr = process.communicate(input=csv_content, timeout=120)
            
            if process.returncode == 0:
                # Extraer n√∫mero de registros insertados
                if "COPY" in stdout:
                    try:
                        inserted_count = int(stdout.split("COPY")[1].strip().split()[0])
                    except:
                        inserted_count = valid_records
                else:
                    inserted_count = valid_records
            else:
                self.log_message(f"‚ùå Error en COPY STDIN: {stderr[:200]}", "ERROR")
                inserted_count = 0
            
            # Estad√≠sticas
            batch_time = time.time() - batch_start
            records_per_second = len(batch_records) / batch_time if batch_time > 0 else 0
            
            self.processed_records += len(batch_records)
            self.successful_inserts += inserted_count
            self.failed_inserts += (len(batch_records) - inserted_count)
            
            progress_percent = (self.processed_records / self.total_records) * 100
            
            self.log_message(f"‚úÖ Lote {batch_number} completado:")
            self.log_message(f"   üìä Procesados: {len(batch_records):,} | Insertados: {inserted_count:,}")
            self.log_message(f"   ‚ö° Tiempo: {batch_time:.1f}s | Velocidad: {records_per_second:.0f} reg/s")
            self.log_message(f"   üìà Progreso: {progress_percent:.2f}% ({self.processed_records:,}/{self.total_records:,})")
            
            return inserted_count
            
        except Exception as e:
            self.log_message(f"‚ùå ERROR en lote {batch_number}: {e}", "ERROR")
            return 0
    
    def method_2_bulk_insert_values(self, batch_number, offset, limit):
        """M√âTODO 2: INSERT masivo con m√∫ltiples VALUES usando archivos SQL"""
        batch_start = time.time()
        total_batches = (self.total_records + self.batch_size - 1) // self.batch_size
        self.log_message(f"üöÄ M√âTODO 2 - Lote {batch_number}/{total_batches} (registros {offset+1:,} - {offset+limit:,})")
        
        try:
            # Obtener datos de SQLite
            conn = sqlite3.connect("numeros.db")
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute(f"SELECT * FROM numeros LIMIT {limit} OFFSET {offset}")
            batch_records = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            if not batch_records:
                return 0
            
            # Transformar registros y crear VALUES masivos
            transformer = DataTransformer()
            values_list = []
            
            for record in batch_records:
                result = transformer.transform_record(record)
                if result["success"]:
                    data = result["data"]
                    
                    def escape_sql(value):
                        if value is None or value == "":
                            return "NULL"
                        elif isinstance(value, str):
                            escaped = value.replace("'", "''").replace("\\", "\\\\")
                            return f"'{escaped}'"
                        elif isinstance(value, bool):
                            return "TRUE" if value else "FALSE"
                        else:
                            return str(value)
                    
                    values_row = f"({escape_sql(data.get('phone_e164'))}, {escape_sql(data.get('phone_national'))}, {escape_sql(data.get('phone_original'))}, {escape_sql(data.get('full_name'))}, {escape_sql(data.get('address'))}, {escape_sql(data.get('neighborhood'))}, {escape_sql(data.get('lada'))}, {escape_sql(data.get('state_code'))}, {escape_sql(data.get('state_name'))}, {escape_sql(data.get('municipality'))}, {escape_sql(data.get('city'))}, {escape_sql(data.get('is_mobile'))}, {escape_sql(data.get('operator'))}, {escape_sql(data.get('status', 'UNKNOWN'))}, NULL, NULL, 0, NULL, NULL, NULL, NULL, 0, {escape_sql(data.get('source', 'TELCEL2022'))}, NULL)"
                    
                    values_list.append(values_row)
            
            if not values_list:
                return 0
            
            # Crear archivo SQL temporal
            with tempfile.NamedTemporaryFile(mode='w', suffix='.sql', delete=False, encoding='utf-8') as temp_file:
                temp_file.write(f"""
BEGIN;
INSERT INTO contacts (
    phone_e164, phone_national, phone_original, full_name, address, neighborhood,
    lada, state_code, state_name, municipality, city, is_mobile, operator,
    status, status_updated_at, status_source, send_count, last_sent_at,
    opt_out_at, opt_out_method, last_validated_at, validation_attempts,
    source, import_batch_id
) VALUES 
{', '.join(values_list)}
ON CONFLICT (phone_e164) DO NOTHING;
COMMIT;
""")
                temp_file_path = temp_file.name
            
            try:
                # Copiar archivo SQL al contenedor
                copy_result = subprocess.run([
                    'docker', 'cp', temp_file_path, 'sms_postgres:/tmp/batch_insert.sql'
                ], capture_output=True, text=True, timeout=30)
                
                if copy_result.returncode != 0:
                    self.log_message(f"‚ùå Error copiando SQL: {copy_result.stderr}", "ERROR")
                    return 0
                
                # Ejecutar archivo SQL
                exec_result = subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-f', '/tmp/batch_insert.sql'
                ], capture_output=True, text=True, timeout=300)
                
                if exec_result.returncode == 0:
                    inserted_count = len(values_list)
                else:
                    self.log_message(f"‚ùå Error ejecutando SQL: {exec_result.stderr[:200]}", "ERROR")
                    inserted_count = 0
                
                # Limpiar archivo del contenedor
                subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'rm', '-f', '/tmp/batch_insert.sql'
                ], capture_output=True)
                
            finally:
                # Limpiar archivo local
                try:
                    os.unlink(temp_file_path)
                except:
                    pass
            
            # Estad√≠sticas
            batch_time = time.time() - batch_start
            records_per_second = len(batch_records) / batch_time if batch_time > 0 else 0
            
            self.processed_records += len(batch_records)
            self.successful_inserts += inserted_count
            self.failed_inserts += (len(batch_records) - inserted_count)
            
            progress_percent = (self.processed_records / self.total_records) * 100
            
            self.log_message(f"‚úÖ Lote {batch_number} completado:")
            self.log_message(f"   üìä Procesados: {len(batch_records):,} | Insertados: {inserted_count:,}")
            self.log_message(f"   ‚ö° Tiempo: {batch_time:.1f}s | Velocidad: {records_per_second:.0f} reg/s")
            self.log_message(f"   üìà Progreso: {progress_percent:.2f}% ({self.processed_records:,}/{self.total_records:,})")
            
            return inserted_count
            
        except Exception as e:
            self.log_message(f"‚ùå ERROR en lote {batch_number}: {e}", "ERROR")
            return 0
    
    def method_3_pg_dump_restore(self):
        """M√âTODO 3: Crear dump SQL completo y restaurar (m√°s r√°pido para datasets grandes)"""
        self.log_message("üöÄ M√âTODO 3: Generando dump SQL completo...")
        
        dump_start = time.time()
        
        try:
            # Crear dump SQL masivo
            dump_path = "migration_dump.sql"
            
            self.log_message("üìù Generando archivo SQL masivo...")
            
            with open(dump_path, 'w', encoding='utf-8') as dump_file:
                dump_file.write("""
-- Migraci√≥n masiva optimizada
BEGIN;

-- Deshabilitar triggers y constraints temporalmente
ALTER TABLE contacts DISABLE TRIGGER ALL;

""")
                
                # Procesar en lotes grandes
                conn = sqlite3.connect("numeros.db")
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                
                transformer = DataTransformer()
                batch_size = 50000
                offset = 0
                
                while True:
                    cursor.execute(f"SELECT * FROM numeros LIMIT {batch_size} OFFSET {offset}")
                    batch_records = [dict(row) for row in cursor.fetchall()]
                    
                    if not batch_records:
                        break
                    
                    values_list = []
                    for record in batch_records:
                        result = transformer.transform_record(record)
                        if result["success"]:
                            data = result["data"]
                            
                            def escape_sql(value):
                                if value is None or value == "":
                                    return "NULL"
                                elif isinstance(value, str):
                                    escaped = value.replace("'", "''").replace("\\", "\\\\")
                                    return f"'{escaped}'"
                                elif isinstance(value, bool):
                                    return "TRUE" if value else "FALSE"
                                else:
                                    return str(value)
                            
                            values_row = f"({escape_sql(data.get('phone_e164'))}, {escape_sql(data.get('phone_national'))}, {escape_sql(data.get('phone_original'))}, {escape_sql(data.get('full_name'))}, {escape_sql(data.get('address'))}, {escape_sql(data.get('neighborhood'))}, {escape_sql(data.get('lada'))}, {escape_sql(data.get('state_code'))}, {escape_sql(data.get('state_name'))}, {escape_sql(data.get('municipality'))}, {escape_sql(data.get('city'))}, {escape_sql(data.get('is_mobile'))}, {escape_sql(data.get('operator'))}, {escape_sql(data.get('status', 'UNKNOWN'))}, NULL, NULL, 0, NULL, NULL, NULL, NULL, 0, {escape_sql(data.get('source', 'TELCEL2022'))}, NULL)"
                            
                            values_list.append(values_row)
                    
                    if values_list:
                        dump_file.write(f"""
INSERT INTO contacts (
    phone_e164, phone_national, phone_original, full_name, address, neighborhood,
    lada, state_code, state_name, municipality, city, is_mobile, operator,
    status, status_updated_at, status_source, send_count, last_sent_at,
    opt_out_at, opt_out_method, last_validated_at, validation_attempts,
    source, import_batch_id
) VALUES 
{', '.join(values_list)};

""")
                    
                    offset += batch_size
                    self.log_message(f"   üìù Procesados {offset:,} registros para dump...")
                
                conn.close()
                
                dump_file.write("""
-- Rehabilitar triggers y constraints
ALTER TABLE contacts ENABLE TRIGGER ALL;

COMMIT;

-- Actualizar estad√≠sticas
ANALYZE contacts;
""")
            
            dump_time = time.time() - dump_start
            dump_size = os.path.getsize(dump_path) / (1024**2)  # MB
            
            self.log_message(f"‚úÖ Dump SQL generado: {dump_path} ({dump_size:.1f}MB) en {dump_time:.1f}s")
            
            # Copiar dump al contenedor
            self.log_message("üìã Copiando dump al contenedor PostgreSQL...")
            copy_result = subprocess.run([
                'docker', 'cp', dump_path, 'sms_postgres:/tmp/migration_dump.sql'
            ], capture_output=True, text=True, timeout=120)
            
            if copy_result.returncode != 0:
                self.log_message(f"‚ùå Error copiando dump: {copy_result.stderr}", "ERROR")
                return False
            
            # Ejecutar dump
            self.log_message("‚ö° Ejecutando migraci√≥n masiva...")
            exec_start = time.time()
            
            exec_result = subprocess.run([
                'docker-compose', 'exec', '-T', 'postgres',
                'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                '-f', '/tmp/migration_dump.sql'
            ], capture_output=True, text=True, timeout=1800)  # 30 minutos timeout
            
            exec_time = time.time() - exec_start
            
            if exec_result.returncode == 0:
                self.log_message(f"‚úÖ Migraci√≥n masiva completada en {exec_time:.1f}s")
                
                # Limpiar archivos
                try:
                    os.unlink(dump_path)
                    subprocess.run([
                        'docker-compose', 'exec', '-T', 'postgres',
                        'rm', '-f', '/tmp/migration_dump.sql'
                    ], capture_output=True)
                except:
                    pass
                
                return True
            else:
                self.log_message(f"‚ùå Error ejecutando dump: {exec_result.stderr[:500]}", "ERROR")
                return False
            
        except Exception as e:
            self.log_message(f"‚ùå ERROR en m√©todo dump: {e}", "ERROR")
            return False
    
    def estimate_completion_time(self):
        """Estimar tiempo restante"""
        if self.processed_records == 0:
            return "Calculando..."
        
        elapsed_time = time.time() - self.start_time
        records_per_second = self.processed_records / elapsed_time
        remaining_records = self.total_records - self.processed_records
        
        if records_per_second > 0:
            remaining_seconds = remaining_records / records_per_second
            completion_time = datetime.now() + timedelta(seconds=remaining_seconds)
            return completion_time.strftime("%Y-%m-%d %H:%M:%S")
        else:
            return "No disponible"
    
    def validate_migration(self):
        """Validar migraci√≥n completa"""
        self.log_message("üîç Validando migraci√≥n completa...")
        
        validation_queries = [
            ("Total registros", "SELECT COUNT(*) FROM contacts"),
            ("N√∫meros E.164 v√°lidos", "SELECT COUNT(*) FROM contacts WHERE phone_e164 ~ '^\\+52[0-9]{10}$'"),
            ("Estados √∫nicos", "SELECT COUNT(DISTINCT state_code) FROM contacts WHERE state_code IS NOT NULL"),
            ("Registros m√≥viles", "SELECT COUNT(*) FROM contacts WHERE is_mobile = true")
        ]
        
        validation_results = {}
        
        for description, query in validation_queries:
            try:
                result = subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-t', '-c', query
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0:
                    count = int(result.stdout.strip())
                    validation_results[description] = count
                    self.log_message(f"   ‚úÖ {description}: {count:,}")
                else:
                    validation_results[description] = 0
            except Exception as e:
                validation_results[description] = 0
        
        return validation_results
    
    def restore_postgresql_config(self):
        """Restaurar configuraci√≥n normal de PostgreSQL"""
        self.log_message("üîß Restaurando configuraci√≥n PostgreSQL...")
        
        restore_queries = [
            "ALTER SYSTEM RESET wal_level;",
            "ALTER SYSTEM RESET synchronous_commit;",
            "ALTER SYSTEM RESET fsync;",
            "ALTER SYSTEM RESET full_page_writes;",
            "ALTER SYSTEM RESET checkpoint_segments;",
            "ALTER SYSTEM RESET autovacuum;",
            "ALTER SYSTEM SET autovacuum = on;",
            "SELECT pg_reload_conf();",
            "VACUUM ANALYZE contacts;"
        ]
        
        for query in restore_queries:
            try:
                subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-c', query
                ], capture_output=True, text=True, timeout=300)
            except:
                pass
        
        self.log_message("‚úÖ Configuraci√≥n PostgreSQL restaurada")
    
    def execute_high_speed_migration(self):
        """Ejecutar migraci√≥n de alta velocidad"""
        self.start_time = time.time()
        
        self.log_message("üöÄ INICIANDO MIGRACI√ìN DE ALTA VELOCIDAD DE 36.6 MILLONES DE REGISTROS")
        self.log_message("=" * 80)
        
        # FASE 1: Preparaci√≥n
        self.log_message("üìã FASE 1: PREPARACI√ìN")
        if not self.check_system_resources():
            return False
        
        if self.get_total_records() == 0:
            return False
        
        self.optimize_postgresql_aggressive()
        
        if not self.clear_target_table():
            return False
        
        # Preguntar m√©todo preferido
        print("\nüöÄ M√âTODOS DE MIGRACI√ìN DISPONIBLES:")
        print("1. COPY desde STDIN (Recomendado - Muy r√°pido)")
        print("2. INSERT masivo con VALUES (R√°pido)")
        print("3. Dump SQL completo (M√°s r√°pido para datasets enormes)")
        
        method_choice = input("\n¬øQu√© m√©todo prefieres? (1/2/3): ").strip()
        
        if method_choice == "3":
            # M√âTODO 3: Dump completo
            self.log_message("\nüì¶ FASE 2: MIGRACI√ìN USANDO DUMP SQL COMPLETO")
            success = self.method_3_pg_dump_restore()
            
            if success:
                # Contar registros insertados
                count_result = subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-t', '-c', 'SELECT COUNT(*) FROM contacts;'
                ], capture_output=True, text=True)
                
                if count_result.returncode == 0:
                    self.successful_inserts = int(count_result.stdout.strip())
                    self.processed_records = self.total_records
        
        else:
            # M√âTODO 1 o 2: Por lotes
            self.log_message(f"\nüì¶ FASE 2: MIGRACI√ìN USANDO M√âTODO {method_choice}")
            
            total_batches = (self.total_records + self.batch_size - 1) // self.batch_size
            self.log_message(f"Procesando {total_batches:,} lotes de {self.batch_size:,} registros cada uno")
            
            for batch_num in range(1, total_batches + 1):
                offset = (batch_num - 1) * self.batch_size
                limit = min(self.batch_size, self.total_records - offset)
                
                if limit <= 0:
                    break
                
                # Seleccionar m√©todo
                if method_choice == "1":
                    inserted = self.method_1_psql_copy_stdin(batch_num, offset, limit)
                else:
                    inserted = self.method_2_bulk_insert_values(batch_num, offset, limit)
                
                # Mostrar progreso cada 5 lotes
                if batch_num % 5 == 0:
                    eta = self.estimate_completion_time()
                    self.log_message(f"üìä PROGRESO: {batch_num}/{total_batches} lotes | ETA: {eta}")
                    
                    memory = psutil.virtual_memory()
                    self.log_message(f"üíæ Memoria: {memory.percent}% usado")
        
        # FASE 3: Validaci√≥n final
        self.log_message("\nüîç FASE 3: VALIDACI√ìN FINAL")
        validation_results = self.validate_migration()
        
        # FASE 4: Optimizaci√≥n post-migraci√≥n
        self.log_message("\n‚ö° FASE 4: OPTIMIZACI√ìN POST-MIGRACI√ìN")
        self.restore_postgresql_config()
        
        # Estad√≠sticas finales
        total_time = time.time() - self.start_time
        records_per_second = self.processed_records / total_time if total_time > 0 else 0
        
        self.log_message("\n" + "=" * 80)
        self.log_message("üéØ MIGRACI√ìN DE ALTA VELOCIDAD COMPLETADA")
        self.log_message("=" * 80)
        self.log_message(f"üì± Registros procesados: {self.processed_records:,}")
        self.log_message(f"‚úÖ Inserciones exitosas: {self.successful_inserts:,}")
        self.log_message(f"‚ùå Inserciones fallidas: {self.failed_inserts:,}")
        self.log_message(f"‚è±Ô∏è  Tiempo total: {total_time/60:.1f} minutos")
        self.log_message(f"üöÄ Velocidad promedio: {records_per_second:.0f} registros/segundo")
        self.log_message(f"üéØ Tasa de √©xito: {(self.successful_inserts/self.processed_records*100):.1f}%")
        
        # Guardar reporte final
        self.save_final_report(validation_results, total_time)
        
        return True
    
    def save_final_report(self, validation_results, total_time):
        """Guardar reporte final de migraci√≥n"""
        report = {
            "migration_summary": {
                "total_records": self.total_records,
                "processed_records": self.processed_records,
                "successful_inserts": self.successful_inserts,
                "failed_inserts": self.failed_inserts,
                "success_rate": (self.successful_inserts/self.processed_records*100) if self.processed_records > 0 else 0,
                "total_time_minutes": total_time/60,
                "records_per_second": self.processed_records/total_time if total_time > 0 else 0
            },
            "validation_results": validation_results,
            "timestamp": datetime.now().isoformat()
        }
        
        report_path = f"MIGRACION_ALTA_VELOCIDAD_36M_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            self.log_message(f"üìÑ Reporte guardado: {report_path}")
        except Exception as e:
            self.log_message(f"‚ö†Ô∏è  Error guardando reporte: {e}", "WARNING")

def main():
    """Funci√≥n principal"""
    solution = HighSpeedMigration()
    
    print("üî• MIGRACI√ìN DE ALTA VELOCIDAD DE 36.6 MILLONES DE REGISTROS")
    print("‚ö° T√âCNICAS PROFESIONALES DE MIGRACI√ìN MASIVA")
    print("‚è±Ô∏è  TIEMPO ESTIMADO: 30 minutos - 2 horas")
    print("üìã M√©todos disponibles:")
    print("   1. COPY desde STDIN - 50,000+ registros/segundo")
    print("   2. INSERT masivo VALUES - 20,000+ registros/segundo") 
    print("   3. Dump SQL completo - 100,000+ registros/segundo")
    print("   4. Optimizaci√≥n agresiva PostgreSQL")
    print("   5. Transacciones masivas optimizadas")
    
    confirm = input("\n¬øContinuar con la migraci√≥n de alta velocidad? (yes/no): ").lower().strip()
    
    if confirm == 'yes':
        success = solution.execute_high_speed_migration()
        
        if success:
            print("\nüéâ ¬°MIGRACI√ìN DE ALTA VELOCIDAD COMPLETADA EXITOSAMENTE!")
            print("üìä Revisa el reporte JSON generado para detalles completos")
        else:
            print("\n‚ùå MIGRACI√ìN FALL√ì - Revisa los logs para detalles")
    else:
        print("\n‚ùå Migraci√≥n cancelada por el usuario")

if __name__ == "__main__":
    main()