#!/usr/bin/env python3
"""
SOLUCI√ìN FINAL DE MIGRACI√ìN - H√çBRIDA
Usa INSERT por lotes peque√±os para evitar todos los problemas de COPY y l√≠mites de Windows
"""

import asyncio
import sqlite3
import subprocess
import sys
import os
import time
import json
import psutil
from datetime import datetime, timedelta
from pathlib import Path

# Agregar el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scripts.migration_manager import DataTransformer

class FinalMigrationSolution:
    """Soluci√≥n final de migraci√≥n usando INSERT por lotes peque√±os"""
    
    def __init__(self):
        self.start_time = None
        self.total_records = 0
        self.processed_records = 0
        self.successful_inserts = 0
        self.failed_inserts = 0
        self.batch_size = 25000  # 25K registros por lote principal
        self.insert_batch_size = 50  # 50 registros por INSERT
        
    def log_message(self, message, level="INFO"):
        """Log con timestamp"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")
        
    def check_system_resources(self):
        """Verificar recursos del sistema"""
        self.log_message("üîç Verificando recursos del sistema...")
        
        memory = psutil.virtual_memory()
        memory_gb = memory.total / (1024**3)
        memory_available_gb = memory.available / (1024**3)
        
        disk = psutil.disk_usage('.')
        disk_free_gb = disk.free / (1024**3)
        
        self.log_message(f"üíæ Memoria: {memory_available_gb:.1f}GB disponible de {memory_gb:.1f}GB total")
        self.log_message(f"üíΩ Disco: {disk_free_gb:.1f}GB libres")
        
        return disk_free_gb >= 40
    
    def backup_source_database(self):
        """Crear backup de numeros.db"""
        self.log_message("üíæ Creando backup de numeros.db...")
        
        source_path = Path("numeros.db")
        if not source_path.exists():
            self.log_message("‚ùå ERROR: numeros.db no encontrado", "ERROR")
            return False
        
        backup_name = f"numeros_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        backup_path = Path("backups") / backup_name
        backup_path.parent.mkdir(exist_ok=True)
        
        try:
            import shutil
            shutil.copy2(source_path, backup_path)
            backup_size = backup_path.stat().st_size
            self.log_message(f"‚úÖ Backup creado: {backup_path} ({backup_size/1024**3:.1f}GB)")
            return True
        except Exception as e:
            self.log_message(f"‚ùå ERROR creando backup: {e}", "ERROR")
            return False
    
    def get_total_records(self):
        """Obtener conteo exacto de registros"""
        self.log_message("üìä Contando registros totales en numeros.db...")
        
        try:
            conn = sqlite3.connect("numeros.db")
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM numeros")
            count = cursor.fetchone()[0]
            conn.close()
            
            self.total_records = count
            total_batches = (count + self.batch_size - 1) // self.batch_size
            
            self.log_message(f"üì± Total de registros: {count:,}")
            self.log_message(f"üì¶ Total de lotes: {total_batches:,} (tama√±o: {self.batch_size:,})")
            
            return count
        except Exception as e:
            self.log_message(f"‚ùå ERROR contando registros: {e}", "ERROR")
            return 0
    
    def optimize_postgresql(self):
        """Optimizar PostgreSQL para inserci√≥n masiva"""
        self.log_message("‚ö° Optimizando PostgreSQL...")
        
        optimization_queries = [
            "ALTER SYSTEM SET synchronous_commit = off;",
            "ALTER SYSTEM SET wal_buffers = '64MB';",
            "ALTER SYSTEM SET checkpoint_completion_target = 0.9;",
            "SELECT pg_reload_conf();"
        ]
        
        for query in optimization_queries:
            try:
                subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-c', query
                ], capture_output=True, text=True, timeout=10)
            except:
                pass
        
        self.log_message("‚úÖ Optimizaci√≥n PostgreSQL completada")
    
    def clear_target_table(self):
        """Limpiar tabla de destino"""
        self.log_message("üßπ Limpiando tabla contacts...")
        
        try:
            result = subprocess.run([
                'docker-compose', 'exec', '-T', 'postgres',
                'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                '-c', 'TRUNCATE TABLE contacts RESTART IDENTITY CASCADE;'
            ], capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                self.log_message("‚úÖ Tabla contacts limpiada")
                return True
            else:
                self.log_message(f"‚ùå Error limpiando tabla: {result.stderr}", "ERROR")
                return False
        except Exception as e:
            self.log_message(f"‚ùå Error limpiando tabla: {e}", "ERROR")
            return False
    
    def migrate_batch_insert(self, batch_number, offset, limit):
        """Migrar un lote usando INSERT por sub-lotes peque√±os"""
        batch_start = time.time()
        total_batches = (self.total_records + self.batch_size - 1) // self.batch_size
        self.log_message(f"üîÑ Procesando lote {batch_number}/{total_batches} (registros {offset+1:,} - {offset+limit:,})")
        
        try:
            # Obtener datos de SQLite
            conn = sqlite3.connect("numeros.db")
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute(f"SELECT * FROM numeros LIMIT {limit} OFFSET {offset}")
            batch_records = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            if not batch_records:
                return 0
            
            # Transformar registros
            transformer = DataTransformer()
            transformed_records = []
            
            for record in batch_records:
                result = transformer.transform_record(record)
                if result["success"]:
                    transformed_records.append(result["data"])
            
            if not transformed_records:
                return 0
            
            # Insertar en sub-lotes peque√±os
            inserted_count = 0
            
            for i in range(0, len(transformed_records), self.insert_batch_size):
                sub_batch = transformed_records[i:i+self.insert_batch_size]
                
                # Crear VALUES para sub-lote
                values_list = []
                for data in sub_batch:
                    def escape_sql(value):
                        if value is None:
                            return "NULL"
                        elif isinstance(value, str):
                            escaped = value.replace("'", "''")
                            return f"'{escaped}'"
                        elif isinstance(value, bool):
                            return "TRUE" if value else "FALSE"
                        else:
                            return str(value)
                    
                    values = f"""(
                        {escape_sql(data.get("phone_e164"))},
                        {escape_sql(data.get("phone_national"))},
                        {escape_sql(data.get("phone_original"))},
                        {escape_sql(data.get("full_name"))},
                        {escape_sql(data.get("address"))},
                        {escape_sql(data.get("neighborhood"))},
                        {escape_sql(data.get("lada"))},
                        {escape_sql(data.get("state_code"))},
                        {escape_sql(data.get("state_name"))},
                        {escape_sql(data.get("municipality"))},
                        {escape_sql(data.get("city"))},
                        {escape_sql(data.get("is_mobile"))},
                        {escape_sql(data.get("operator"))},
                        {escape_sql(data.get("status", "UNKNOWN"))},
                        NULL,
                        NULL,
                        0,
                        NULL,
                        NULL,
                        NULL,
                        NULL,
                        0,
                        {escape_sql(data.get("source", "TELCEL2022"))},
                        NULL
                    )"""
                    values_list.append(values)
                
                # Crear comando INSERT
                insert_sql = f"""
                INSERT INTO contacts (
                    phone_e164, phone_national, phone_original, full_name, address, neighborhood,
                    lada, state_code, state_name, municipality, city, is_mobile, operator,
                    status, status_updated_at, status_source, send_count, last_sent_at,
                    opt_out_at, opt_out_method, last_validated_at, validation_attempts,
                    source, import_batch_id
                ) VALUES {', '.join(values_list)}
                ON CONFLICT (phone_e164) DO NOTHING;
                """
                
                try:
                    result = subprocess.run([
                        'docker-compose', 'exec', '-T', 'postgres',
                        'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                        '-c', insert_sql
                    ], capture_output=True, text=True, timeout=30)
                    
                    if result.returncode == 0:
                        # Contar inserciones del output
                        if "INSERT 0" in result.stdout:
                            try:
                                insert_count = int(result.stdout.split("INSERT 0")[1].strip().split()[0])
                                inserted_count += insert_count
                            except:
                                inserted_count += len(sub_batch)
                    else:
                        self.log_message(f"‚ö†Ô∏è  Error en sub-lote: {result.stderr[:100]}", "WARNING")
                        
                except subprocess.TimeoutExpired:
                    self.log_message(f"‚ö†Ô∏è  Timeout en sub-lote", "WARNING")
                except Exception as e:
                    self.log_message(f"‚ö†Ô∏è  Excepci√≥n en sub-lote: {e}", "WARNING")
            
            # Estad√≠sticas del lote
            batch_time = time.time() - batch_start
            records_per_second = len(batch_records) / batch_time if batch_time > 0 else 0
            
            self.processed_records += len(batch_records)
            self.successful_inserts += inserted_count
            self.failed_inserts += (len(batch_records) - inserted_count)
            
            progress_percent = (self.processed_records / self.total_records) * 100
            
            self.log_message(f"‚úÖ Lote {batch_number} completado:")
            self.log_message(f"   üìä Procesados: {len(batch_records):,} | Insertados: {inserted_count:,}")
            self.log_message(f"   ‚è±Ô∏è  Tiempo: {batch_time:.1f}s | Velocidad: {records_per_second:.0f} reg/s")
            self.log_message(f"   üìà Progreso total: {progress_percent:.1f}% ({self.processed_records:,}/{self.total_records:,})")
            
            return inserted_count
            
        except Exception as e:
            self.log_message(f"‚ùå ERROR en lote {batch_number}: {e}", "ERROR")
            return 0
    
    def estimate_completion_time(self):
        """Estimar tiempo restante"""
        if self.processed_records == 0:
            return "Calculando..."
        
        elapsed_time = time.time() - self.start_time
        records_per_second = self.processed_records / elapsed_time
        remaining_records = self.total_records - self.processed_records
        
        if records_per_second > 0:
            remaining_seconds = remaining_records / records_per_second
            completion_time = datetime.now() + timedelta(seconds=remaining_seconds)
            return completion_time.strftime("%Y-%m-%d %H:%M:%S")
        else:
            return "No disponible"
    
    def validate_migration(self):
        """Validar migraci√≥n completa"""
        self.log_message("üîç Validando migraci√≥n completa...")
        
        validation_queries = [
            ("Total registros", "SELECT COUNT(*) FROM contacts"),
            ("N√∫meros E.164 v√°lidos", "SELECT COUNT(*) FROM contacts WHERE phone_e164 ~ '^\\+52[0-9]{10}$'"),
            ("Estados √∫nicos", "SELECT COUNT(DISTINCT state_code) FROM contacts WHERE state_code IS NOT NULL"),
            ("Registros m√≥viles", "SELECT COUNT(*) FROM contacts WHERE is_mobile = true")
        ]
        
        validation_results = {}
        
        for description, query in validation_queries:
            try:
                result = subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-t', '-c', query
                ], capture_output=True, text=True, timeout=30)
                
                if result.returncode == 0:
                    count = int(result.stdout.strip())
                    validation_results[description] = count
                    self.log_message(f"   ‚úÖ {description}: {count:,}")
                else:
                    validation_results[description] = 0
            except Exception as e:
                validation_results[description] = 0
        
        return validation_results
    
    def restore_postgresql_config(self):
        """Restaurar configuraci√≥n normal de PostgreSQL"""
        self.log_message("üîß Restaurando configuraci√≥n PostgreSQL...")
        
        restore_queries = [
            "ALTER SYSTEM RESET synchronous_commit;",
            "ALTER SYSTEM RESET wal_buffers;",
            "ALTER SYSTEM RESET checkpoint_completion_target;",
            "SELECT pg_reload_conf();",
            "VACUUM ANALYZE contacts;"
        ]
        
        for query in restore_queries:
            try:
                subprocess.run([
                    'docker-compose', 'exec', '-T', 'postgres',
                    'psql', '-U', 'sms_user', '-d', 'sms_marketing',
                    '-c', query
                ], capture_output=True, text=True, timeout=300)
            except:
                pass
        
        self.log_message("‚úÖ Configuraci√≥n PostgreSQL restaurada")
    
    async def execute_final_migration(self):
        """Ejecutar migraci√≥n completa usando soluci√≥n h√≠brida"""
        self.start_time = time.time()
        
        self.log_message("üöÄ INICIANDO MIGRACI√ìN FINAL DE 36.6 MILLONES DE REGISTROS")
        self.log_message("=" * 80)
        
        # FASE 1: Preparaci√≥n
        self.log_message("üìã FASE 1: PREPARACI√ìN")
        if not self.check_system_resources():
            return False
        
        if not self.backup_source_database():
            return False
        
        if self.get_total_records() == 0:
            return False
        
        self.optimize_postgresql()
        
        if not self.clear_target_table():
            return False
        
        # FASE 2: Migraci√≥n por lotes usando INSERT h√≠brido
        self.log_message("\nüì¶ FASE 2: MIGRACI√ìN POR LOTES (M√âTODO INSERT H√çBRIDO)")
        
        total_batches = (self.total_records + self.batch_size - 1) // self.batch_size
        self.log_message(f"Procesando {total_batches:,} lotes de {self.batch_size:,} registros cada uno")
        
        for batch_num in range(1, total_batches + 1):
            offset = (batch_num - 1) * self.batch_size
            limit = min(self.batch_size, self.total_records - offset)
            
            if limit <= 0:
                break
            
            # Migrar lote
            inserted = self.migrate_batch_insert(batch_num, offset, limit)
            
            # Mostrar progreso cada 5 lotes
            if batch_num % 5 == 0:
                eta = self.estimate_completion_time()
                self.log_message(f"üìä PROGRESO: {batch_num}/{total_batches} lotes | ETA: {eta}")
                
                memory = psutil.virtual_memory()
                self.log_message(f"üíæ Memoria: {memory.percent}% usado")
        
        # FASE 3: Validaci√≥n final
        self.log_message("\nüîç FASE 3: VALIDACI√ìN FINAL")
        validation_results = self.validate_migration()
        
        # FASE 4: Optimizaci√≥n post-migraci√≥n
        self.log_message("\n‚ö° FASE 4: OPTIMIZACI√ìN POST-MIGRACI√ìN")
        self.restore_postgresql_config()
        
        # Estad√≠sticas finales
        total_time = time.time() - self.start_time
        records_per_second = self.processed_records / total_time if total_time > 0 else 0
        
        self.log_message("\n" + "=" * 80)
        self.log_message("üéØ MIGRACI√ìN FINAL COMPLETADA")
        self.log_message("=" * 80)
        self.log_message(f"üì± Registros procesados: {self.processed_records:,}")
        self.log_message(f"‚úÖ Inserciones exitosas: {self.successful_inserts:,}")
        self.log_message(f"‚ùå Inserciones fallidas: {self.failed_inserts:,}")
        self.log_message(f"‚è±Ô∏è  Tiempo total: {total_time/3600:.1f} horas")
        self.log_message(f"üöÄ Velocidad promedio: {records_per_second:.0f} registros/segundo")
        self.log_message(f"üéØ Tasa de √©xito: {(self.successful_inserts/self.processed_records*100):.1f}%")
        
        # Guardar reporte final
        self.save_final_report(validation_results, total_time)
        
        return True
    
    def save_final_report(self, validation_results, total_time):
        """Guardar reporte final de migraci√≥n"""
        report = {
            "migration_summary": {
                "total_records": self.total_records,
                "processed_records": self.processed_records,
                "successful_inserts": self.successful_inserts,
                "failed_inserts": self.failed_inserts,
                "success_rate": (self.successful_inserts/self.processed_records*100) if self.processed_records > 0 else 0,
                "total_time_hours": total_time/3600,
                "records_per_second": self.processed_records/total_time if total_time > 0 else 0
            },
            "validation_results": validation_results,
            "timestamp": datetime.now().isoformat()
        }
        
        report_path = f"MIGRACION_FINAL_36M_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            self.log_message(f"üìÑ Reporte guardado: {report_path}")
        except Exception as e:
            self.log_message(f"‚ö†Ô∏è  Error guardando reporte: {e}", "WARNING")

async def main():
    """Funci√≥n principal"""
    solution = FinalMigrationSolution()
    
    print("üî• MIGRACI√ìN FINAL DE 36.6 MILLONES DE REGISTROS")
    print("üõ†Ô∏è  SOLUCI√ìN H√çBRIDA - INSERT POR LOTES PEQUE√ëOS")
    print("‚ö†Ô∏è  ADVERTENCIA: Esta operaci√≥n puede tomar 8-12 horas")
    print("üìã Caracter√≠sticas de esta soluci√≥n:")
    print("   - INSERT por lotes de 50 registros (evita l√≠mites de Windows)")
    print("   - Lotes principales de 25K registros")
    print("   - Manejo completo de todas las columnas")
    print("   - ON CONFLICT para evitar duplicados")
    print("   - Monitoreo en tiempo real")
    
    confirm = input("\n¬øContinuar con la migraci√≥n final? (yes/no): ").lower().strip()
    
    if confirm == 'yes':
        success = await solution.execute_final_migration()
        
        if success:
            print("\nüéâ ¬°MIGRACI√ìN FINAL COMPLETADA EXITOSAMENTE!")
            print("üìä Revisa el reporte JSON generado para detalles completos")
        else:
            print("\n‚ùå MIGRACI√ìN FALL√ì - Revisa los logs para detalles")
    else:
        print("\n‚ùå Migraci√≥n cancelada por el usuario")

if __name__ == "__main__":
    asyncio.run(main())